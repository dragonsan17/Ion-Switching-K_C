{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Hi everyone!\n",
    "\n",
    "This is my first Kaggle Competition and Kernel. I tried working with Support Vector Machines, and achieved very high F1 macro score with the same. I am sharing my results below.\n",
    "Dataset used : https://www.kaggle.com/cdeotte/data-without-drift\n",
    "\n",
    "I have used 5 different SVM models. For more details and detailed plots, go here: https://www.kaggle.com/cdeotte/one-feature-model-0-930/output\n",
    "\n",
    "The above link explains how 5 different models were used to create synthetic data.\n",
    "\n",
    "I do not have much experience with Machine Learning, so I have naturally explained in a very simpler manner. Enjoy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "The below cell will input data and store them in numpy arrays. There are 5 models: Model 0, Model 1...Model 4. They are our estimations of the original respective models used to generate respective batches:\n",
    "\n",
    "1. Model 0: \n",
    "    * **Training Batches** 0,1\n",
    "    * **Testing Batches** 0,3,8,10,11,12,13,14,15,16,17,18,19\n",
    "    * **Maximum Open Channels**: 1\n",
    "2. Model 1: \n",
    "    * **Training Batches** 2,6\n",
    "    * **Testing Batches** 4\n",
    "    * **Maximum Open Channels**: 1\n",
    "3. Model 2: \n",
    "    * **Training Batches** 3,7\n",
    "    * **Testing Batches** 1,9\n",
    "    * **Maximum Open Channels**: 3\n",
    "4. Model 3: \n",
    "    * **Training Batches** 4,9\n",
    "    * **Testing Batches** 5,7\n",
    "    * **Maximum Open Channels**: 10\n",
    "5. Model 4: \n",
    "    * **Training Batches** 5,8\n",
    "    * **Testing Batches** 2,6\n",
    "    * **Maximum Open Channels**: 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/kaggle/input/data-without-drift/'\n",
    "train_data_file = data_path + 'train_clean.csv'\n",
    "test_data_file = data_path + 'test_clean.csv'\n",
    "\n",
    "def get_data(filename, train=True):\n",
    "  \n",
    "    if(train):\n",
    "        with open(filename) as training_file:\n",
    "            split_size = 10\n",
    "            data = np.loadtxt(training_file, delimiter=',', skiprows=1)\n",
    "            signal = data[:,1]\n",
    "            channels = data[:,2]\n",
    "            signal = np.array_split(signal, split_size)\n",
    "            channels = np.array_split(channels, split_size)\n",
    "            data = None\n",
    "        return np.array(signal), np.array(channels)\n",
    "    else:\n",
    "       with open(filename) as training_file:\n",
    "            split_size = 4\n",
    "            data = np.loadtxt(training_file, delimiter=',', skiprows=1)\n",
    "            signal = data[:,1]\n",
    "            signal = np.array_split(signal, split_size)\n",
    "            data = None\n",
    "       return np.array(signal)\n",
    "\n",
    "train_signal , train_channels = get_data(train_data_file)\n",
    "test_signal = get_data(test_data_file, train=False)\n",
    "\n",
    "test_model_signal = np.zeros((5,1000000))\n",
    "test_model_channel = np.zeros((5,1000000))\n",
    "test_model_signal[0][:500000] = train_signal[0].flatten()\n",
    "test_model_signal[0][500000:] = train_signal[1].flatten()\n",
    "test_model_signal[1][:500000] = train_signal[2].flatten()\n",
    "test_model_signal[1][500000:] = train_signal[6].flatten()\n",
    "test_model_signal[2][:500000] = train_signal[3].flatten()\n",
    "test_model_signal[2][500000:] = train_signal[7].flatten()\n",
    "test_model_signal[3][:500000] = train_signal[4].flatten()\n",
    "test_model_signal[3][500000:] = train_signal[9].flatten()\n",
    "test_model_signal[4][:500000] = train_signal[5].flatten()\n",
    "test_model_signal[4][500000:] = train_signal[8].flatten()\n",
    "\n",
    "\n",
    "test_model_channel[0][:500000] = train_channels[0].flatten()\n",
    "test_model_channel[0][500000:] = train_channels[1].flatten()\n",
    "test_model_channel[1][:500000] = train_channels[2].flatten()\n",
    "test_model_channel[1][500000:] = train_channels[6].flatten()\n",
    "test_model_channel[2][:500000] = train_channels[3].flatten()\n",
    "test_model_channel[2][500000:] = train_channels[7].flatten()\n",
    "test_model_channel[3][:500000] = train_channels[4].flatten()\n",
    "test_model_channel[3][500000:] = train_channels[9].flatten()\n",
    "test_model_channel[4][:500000] = train_channels[5].flatten()\n",
    "test_model_channel[4][500000:] = train_channels[8].flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Specs below refers to specifications of SVM model, namely C and gamma. You need to have a basic understanding of what an SVM is to understand the math behind the specifications. These were evaluated using a grid search for hyperparameter tuning. Refer to documentation of sklearn.svm.svc for more details. \n",
    "\n",
    "Below, the model is trained on the first 400000 entries and validated on the next 100000 entries. The remaining 500000 is unused. You can do undersampling and upsampling to generate a well balanced data but the below also works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting training model no:  0\n",
      "[0.99942855 0.97204194]\n",
      "0.9857352455148416\n",
      "starting training model no:  1\n",
      "[0.98950849 0.99642754]\n",
      "0.9929680178508269\n",
      "starting training model no:  2\n",
      "[0.97255454 0.97828286 0.9816969  0.98735409]\n",
      "0.9799720959170334\n",
      "starting training model no:  3\n",
      "[0.8        0.82511211 0.84218399 0.8534202  0.86819845 0.87489464\n",
      " 0.87845751 0.8801643  0.88030013 0.87362792]\n",
      "0.8576359244670078\n",
      "starting training model no:  4\n",
      "[0.94567404 0.95925495 0.96636798 0.97028753 0.97310097 0.9759043 ]\n",
      "0.9650982952316117\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "models = []\n",
    "\n",
    "specs = [[1.2,1],[0.1,1],[0.5,1],[7,0.01],[10,0.1]]\n",
    "\n",
    "for k in range (5):\n",
    "    print(\"starting training model no: \", k)\n",
    "    x = test_model_signal[k].flatten()\n",
    "    y = test_model_channel[k].flatten()\n",
    "    y = np.array(y).astype(int)\n",
    "    x = np.expand_dims(np.array(x),-1)\n",
    "    model = SVC(kernel = 'rbf', C=specs[k][0],gamma = specs[k][1])\n",
    "    samples= 400000\n",
    "    #trains by splitting into 10 batches for faster training\n",
    "    for i in range(10):\n",
    "        model.fit(x[i*samples//10:(i+1)*samples//10],y[i*samples//10:(i+1)*samples//10])\n",
    "    y_pred = model.predict(x[400000:500000])\n",
    "    y_true = y[400000:500000]\n",
    "    print(f1_score(y_true, y_pred, average=None))\n",
    "    print(f1_score(y_true, y_pred, average='macro'))\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "The following is the testing process. Each batch is of length 100000, which can be easily seen from plotting the signal values. The model for each batch can be manually determined, or by calculating the average of all the entries on each batch and matching the same with the average of training batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting prediction of test batch no:  0\n",
      "starting prediction of test batch no:  1\n",
      "starting prediction of test batch no:  2\n",
      "starting prediction of test batch no:  3\n",
      "starting prediction of test batch no:  4\n",
      "starting prediction of test batch no:  5\n",
      "starting prediction of test batch no:  6\n",
      "starting prediction of test batch no:  7\n",
      "starting prediction of test batch no:  8\n",
      "starting prediction of test batch no:  9\n",
      "starting prediction of test batch no:  10\n",
      "starting prediction of test batch no:  11\n",
      "starting prediction of test batch no:  12\n",
      "starting prediction of test batch no:  13\n",
      "starting prediction of test batch no:  14\n",
      "starting prediction of test batch no:  15\n",
      "starting prediction of test batch no:  16\n",
      "starting prediction of test batch no:  17\n",
      "starting prediction of test batch no:  18\n",
      "starting prediction of test batch no:  19\n"
     ]
    }
   ],
   "source": [
    "model_ref = [0,2,4,0,1,3,4,3,0,2,0,0,0,0,0,0,0,0,0,0]\n",
    "y_pred_all = np.zeros((2000000))\n",
    "for pec in range(20):\n",
    "  print(\"starting prediction of test batch no: \", pec)\n",
    "  x_test = test_signal.flatten()[pec*100000:(pec+1)*100000]\n",
    "  x_test = np.expand_dims(np.array(x_test),-1)\n",
    "  test_pred = models[model_ref[pec]].predict(x_test)\n",
    "  y_pred_1 = np.array(test_pred).astype(int)\n",
    "  y_pred_all[pec*100000:(pec+1)*100000] = y_pred_1\n",
    "\n",
    "y_pred_all = np.array(y_pred_all).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "The following is a good estimation of LB. As it is known that the first 600000 or the first 6 batches of testing data are used for the public leaderboard. So, we evaluate the results for 6 batches of validation data from similar models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99917212 0.99094251 0.9779509  0.97846288 0.9647749  0.94008288\n",
      " 0.87489464 0.87845751 0.8801643  0.88030013 0.87362792]\n",
      "0.9308027905289973\n"
     ]
    }
   ],
   "source": [
    "model_ref = [0,0,1,2,3,4]\n",
    "y_valid = np.zeros((1000000))\n",
    "y_pred = np.zeros((1000000))\n",
    "for k in range(6):\n",
    "  x = train_signal[k].flatten()\n",
    "  y = train_channels[k].flatten()\n",
    "  y = np.array(y).astype(int)\n",
    "  x = np.expand_dims(np.array(x),-1)\n",
    "  model = models[model_ref[k]]\n",
    "  y_pred[k*100000:(k+1)*100000] = model.predict(x[400000:500000])\n",
    "  y_valid[k*100000:(k+1)*100000]=y[400000:500000]\n",
    "\n",
    "print(f1_score(y_valid, y_pred, average=None))\n",
    "print(f1_score(y_valid, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "The following writes the testing predictions into csv file for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_kg_hide-output": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved the file\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "sub = pd.read_csv('/kaggle/input/liverpool-ion-switching/sample_submission.csv')\n",
    "sub.iloc[:,1] = y_pred_all\n",
    "sub.to_csv('submission.csv',index=False,float_format='%.4f')\n",
    "print(\"saved the file\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
